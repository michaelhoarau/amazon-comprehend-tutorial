{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom entity recognition with Comprehend\n",
    "\n",
    "*Step 1: preparing the training dataset for custom entity recognition*\n",
    "\n",
    "\n",
    "This series of notebook is a walkthrough on how to leverage Amazon Comprehend to recognize customized entities from documents. More details about the training process can be found here: https://docs.aws.amazon.com/comprehend/latest/dg/training-recognizers.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "---\n",
    "### Update SageMaker execution role credentials\n",
    "Before you start make sure that your Sagemaker Execution Role has the following credentials.\n",
    "\n",
    "* `ComprehendFullAccess`\n",
    "* `ComprehendMedicalFullAccess`\n",
    "* `TranslateFullAccess`\n",
    "* `SagemakerFullAccess`\n",
    "* Your Sagemaker Execution Role should have access to S3 already. If not you can add the `S3FullAccess` policy.\n",
    "* You will also need to add `iam:passRole` as an inline policy.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Action\": [\n",
    "                \"iam:PassRole\"\n",
    "            ],\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "![Policies](../assets/iam-policies.png)\n",
    "\n",
    "* You will also need the following trust policies:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"Service\": [\n",
    "          \"sagemaker.amazonaws.com\",\n",
    "          \"s3.amazonaws.com\",\n",
    "          \"comprehend.amazonaws.com\"\n",
    "        ]\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "![Trust Relationships](../assets/iam-trust-relationships.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "RAW_DATA = '../data/raw'\n",
    "DATA = '../data/interim'\n",
    "PROCESSED_DATA = '../data/processed'\n",
    "\n",
    "os.makedirs(DATA, exist_ok=True)\n",
    "os.makedirs(RAW_DATA, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DATA, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import sys\n",
    "\n",
    "# Specify S3 bucket and prefix that you want to use for the model data\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'comprehend_workshop'\n",
    "execution_role = sagemaker.get_execution_role()\n",
    "\n",
    "# Check if the bucket exists\n",
    "try:\n",
    "    boto3.Session().client('s3').head_bucket(Bucket=bucket)\n",
    "except botocore.exceptions.ParamValidationError as e:\n",
    "    print('Hey! You either forgot to specify your S3 bucket'\n",
    "          ' or you gave your bucket an invalid name!')\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == '403':\n",
    "        print(\"Hey! You don't have permission to access the bucket, {}.\".format(bucket))\n",
    "    elif e.response['Error']['Code'] == '404':\n",
    "        print(\"Hey! Your bucket, {}, doesn't exist!\".format(bucket))\n",
    "    else:\n",
    "        raise\n",
    "else:\n",
    "    print('Training input/output will be stored in: s3://{}/{}'.format(bucket, prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading datasets\n",
    "For this training, we will use the Quaero French Medical corpus dataset that can be downloaded from here: https://quaerofrenchmed.limsi.fr/QUAERO_FrenchMed_brat.zip. For this dataset, a selection of MEDLINE titles and EMEA documents were manually annotated. The annotation process was guided by concepts in the Unified Medical Language System (UMLS). As such, ten types of clinical entities, as defined by the following UMLS Semantic Groups (Bodenreider and McCray 2003) were annotated: **Anatomy**, **Chemical and Drugs**, **Devices**, **Disorders**, **Geographic Areas**, **Living Beings**, **Objects**, **Phenomena**, **Physiology** and **Procedures**. Amazon Comprehend Medical follows a similar semantic and was trained on content in the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if os.path.exists(os.path.join(RAW_DATA, 'corpus')):\n",
    "    print('Dataset was apparently already downloaded, nothing to do here.')\n",
    "    \n",
    "else:\n",
    "    print('Dataset not found, downloading and unzipping content.')\n",
    "    !wget https://quaerofrenchmed.limsi.fr/QUAERO_FrenchMed_brat.zip -O $RAW_DATA/QUAERO_FrenchMed_brat.zip\n",
    "    !unzip -qq $RAW_DATA/QUAERO_FrenchMed_brat.zip -d $RAW_DATA\n",
    "    !mv $RAW_DATA/QUAERO_FrenchMed/corpus $RAW_DATA\n",
    "    !rm -Rf $RAW_DATA/QUAERO_FrenchMed\n",
    "    !rm $RAW_DATA/QUAERO_FrenchMed_brat.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_int(s):\n",
    "    \"\"\"\n",
    "    Checks if the string argument contains an int\n",
    "    \n",
    "    :param: s - string that we want to test the content for\n",
    "    \"\"\"\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary of entities we will capture: the entity types **must** be in uppercase for the custom entity training on Amazon Comprehend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = {\n",
    "    'ANAT': 'ANATOMY',\n",
    "    'CHEM': 'CHEMICALS',\n",
    "    'DEVI': 'DEVICES',\n",
    "    'DISO': 'DISORDERS',\n",
    "    'GEOG': 'GEOGRAPHIC_AREA',\n",
    "    'LIVB': 'LIVING_BEING',\n",
    "    'OBJC': 'OBJECT',\n",
    "    'PHEN': 'PHENOMENON',\n",
    "    'PHYS': 'PHYSIOLOGY',\n",
    "    'PROC': 'PROCEDURE'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "---\n",
    "The Quaero corpus uses the BRAT format:\n",
    "Document example in `filename.txt`:\n",
    "\n",
    "```\n",
    "La contraception par les dispositifs intra utérins\n",
    "```\n",
    "\n",
    "Associated annotations in `filename.ann`:\n",
    "```\n",
    "T1 PROC 3 16 contraception\n",
    "#1 AnnotatorNotes T1 C0700589\n",
    "T2 DEVI 25 50 dispositifs intra utérins\n",
    "#2 AnnotatorNotes T2 C0021900\n",
    "T3 ANAT 43 50 utérins\n",
    "#3 AnnotatorNotes T3 C0042149\n",
    "```\n",
    "\n",
    "Note that:\n",
    "* Several annotations can be associated to each line.\n",
    "* The begin and end offset are **relative to the whole file**, and not to the current line.\n",
    "* Documents samples can contain empty lines (with \\n characters to take into account when looking for the correct offsets for the tagged entities).\n",
    "* Some files can contain several rows: we will consider each row as an individual document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# We will process the whole corpus data, as the Comprehend training already includes a train/test split procedure internally.\n",
    "root_dir = os.path.join(RAW_DATA, 'corpus')\n",
    "training_doc = pd.DataFrame(columns=['File', 'Line', 'Begin Offset', 'End Offset', 'Type'])\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# We walk throughout the directory and process all the .txt (document) and .ann (annotations) files\n",
    "index = 0\n",
    "for root, dirs, files in os.walk(top=root_dir):\n",
    "    # Loops through the files of the current directory:\n",
    "    for name in tqdm(files, desc=root):\n",
    "        # We only process the document samples:\n",
    "        ext = name[-4:]\n",
    "        if ext == '.txt':\n",
    "            # Read each line of the current documents:\n",
    "            document_fname = os.path.join(root, name)\n",
    "            with open(document_fname, 'r') as document:\n",
    "                content = document.readlines()\n",
    "                \n",
    "            # The BRAT format registers position in the whole \n",
    "            # file: we want to use positions in each current \n",
    "            # line: let's loop through each row of the document:\n",
    "            last_position = 0\n",
    "            df = pd.DataFrame(columns=['document', 'start_pos'])\n",
    "            for index, row in enumerate(content):\n",
    "                # Compute the starting position of the current row relative to the document:\n",
    "                start = last_position\n",
    "                length = len(row)\n",
    "                last_position += length\n",
    "\n",
    "                # This dataframe will record each row (individual documents) and \n",
    "                # its starting position relative to the file currently opened:\n",
    "                df = df.append({\n",
    "                    'document': row,\n",
    "                    'start_pos': start,\n",
    "                }, ignore_index=True)\n",
    "\n",
    "                # Ignore empty lines to filter the right documents to be sent to S3:\n",
    "                if row != '\\n':\n",
    "                    # Each row contained in the file is sent as an individual document to S3.\n",
    "                    new_document_fname = os.path.join(PROCESSED_DATA, name[:-4] + '_row{}.txt'.format(index))\n",
    "                    with open(new_document_fname, 'w') as new_document:\n",
    "                        new_document.write(row)    \n",
    "                    s3.meta.client.upload_file(new_document_fname, bucket, prefix + '/documents/' + name[:-4] + '_row{}.txt'.format(index))\n",
    "                    \n",
    "            # We now have a dataframe registering all the tags \n",
    "            # found in the current file. Let's process the \n",
    "            # associated annotations file:\n",
    "            annotation_fname = os.path.join(root, name[:-4] + '.ann')\n",
    "            with open(annotation_fname, 'r') as f:\n",
    "                annotations = f.readlines()\n",
    "                \n",
    "            # We only loop through annotations lines that do not start by #:\n",
    "            annotations = [a for a in annotations if a[0] != '#']\n",
    "            for a in annotations:\n",
    "                # Split each annotation line following this scheme: \n",
    "                # TYPE BEGIN_OFFSET END_OFFSET EXPRESSION\n",
    "                a = a.split('\\t')[1].split(' ')\n",
    "\n",
    "                # Some annotations have more complex scheme, \n",
    "                # but they are rare. We will discard them at \n",
    "                # this stage and only keep the ones with begin\n",
    "                # and end offset stored as mere integers:\n",
    "                if is_int(a[1]) and is_int(a[2]):\n",
    "                    # Extract information from the current annotation line:\n",
    "                    start = int(a[1])\n",
    "                    end = int(a[2])\n",
    "                    \n",
    "                    # Where was this annotation positioned in the file?\n",
    "                    doc_info = df[df['start_pos'] <= start].iloc[-1]\n",
    "                    \n",
    "                    # This is the row number:\n",
    "                    doc_id = doc_info.name\n",
    "                    \n",
    "                    # Compute start and end of current entity relative to its line, not to the whole file:\n",
    "                    entity_start = start - doc_info['start_pos']\n",
    "                    entity_end = end - doc_info['start_pos']\n",
    "\n",
    "                    # Build the current training sample\n",
    "                    training_doc = training_doc.append({\n",
    "                        'File': name[:-4] + '_row{}.txt'.format(doc_id),\n",
    "                        'Line': 0,\n",
    "                        'Begin Offset': entity_start,\n",
    "                        'End Offset': entity_end,\n",
    "                        'Type': types[a[0]]\n",
    "                    }, ignore_index=True)\n",
    "\n",
    "print(training_doc.shape)\n",
    "training_doc.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Quaero corpus **contains overlap** which Amazon Comprehend NER training feature **does not accept**, let's filter them out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_doc['Begin Offset'] = training_doc['Begin Offset'].astype(np.int16)\n",
    "training_doc['End Offset'] = training_doc['End Offset'].astype(np.int16)\n",
    "nb_overlaps = 1\n",
    "pass_index = 0\n",
    "\n",
    "# Loops through this process until no more duplicate are found:\n",
    "while nb_overlaps > 0:\n",
    "    pass_index += 1\n",
    "    training_doc['overlap'] = (training_doc.groupby(by='File').apply(lambda x: (x['End Offset'].shift() - x['Begin Offset']) > 0).reset_index(level=0, drop=True))\n",
    "    nb_overlaps = training_doc[training_doc['overlap'] == True].shape[0]\n",
    "    print('Pass {}: {} overlaps removed'.format(pass_index, nb_overlaps))\n",
    "    training_doc = training_doc[training_doc['overlap'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Comprehend needs at least 200 samples for each entity type, let's filter out the documents pertaining to samples with an insufficient cardinality before pushing our final dataset to S3. The dataset must have the following format (pay attention to save the CSV file **without any dataframe index** and double check the column names):\n",
    "\n",
    "<img src=\"../assets/comprehend_training_format.png\" width=\"370\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep documents for entity types with more than 200 samples:\n",
    "counts = training_doc['Type'].value_counts()\n",
    "types_list = counts[counts >= 200].index.tolist()\n",
    "training_doc = training_doc[training_doc['Type'].isin(types_list)]\n",
    "print(f'{training_doc.shape[0]} documents left in the training dataset')\n",
    "\n",
    "# Push the final training dataset to S3.\n",
    "training_doc = training_doc[['File', 'Line', 'Begin Offset', 'End Offset', 'Type']]\n",
    "training_doc.to_csv(os.path.join(PROCESSED_DATA, 'annotations.csv'), index=False)\n",
    "s3.meta.client.upload_file(os.path.join(PROCESSED_DATA, 'annotations.csv'), bucket, prefix + '/annotations/' + 'annotations.csv')\n",
    "\n",
    "# Print the document number per entity type:\n",
    "counts[counts > 200]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
